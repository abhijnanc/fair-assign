{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKUbySV1laji"
      },
      "outputs": [],
      "source": [
        "#Importing all the libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import csv\n",
        "import ortools\n",
        "import math\n",
        "import configparser\n",
        "import datetime\n",
        "import time\n",
        "from geopy.geocoders import Nominatim\n",
        "matplotlib.rc('xtick', labelsize=26) \n",
        "matplotlib.rc('ytick', labelsize=26) \n",
        "plt.rcParams['font.size'] = '26'\n",
        "plt.rcParams['figure.figsize'] = (10,7.5)\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(10, 7.5)\n",
        "\n",
        "\n",
        "configParser = configparser.RawConfigParser()   \n",
        "configFilePath = 'config.cfg'\n",
        "configParser.read(configFilePath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPc-NN5Dj1DX",
        "outputId": "a2f6f457-c0b3-49e2-b102-ac2a224770b4"
      },
      "outputs": [],
      "source": [
        "#File paths\n",
        "configParser.read(configFilePath)\n",
        "customer_dir=configParser.get('file-paths','customer_dir')\n",
        "geolocation_dir=configParser.get('file-paths','geolocation_dir')\n",
        "seller_dir = configParser.get('file-paths','seller_dir')\n",
        "order_dir= configParser.get('file-paths','order_dir')\n",
        "order_item_dir=configParser.get('file-paths','order_item_dir')\n",
        "\n",
        "\n",
        "#Creating dataframes\n",
        "customer_df=pd.read_csv(customer_dir)\n",
        "location_df=pd.read_csv(geolocation_dir)\n",
        "seller_df=pd.read_csv(seller_dir)\n",
        "order_df=pd.read_csv(order_dir)\n",
        "order_item_df=pd.read_csv(order_item_dir)\n",
        "\n",
        "# location_df=location_df[location_df['geolocation_state']=='SP']\n",
        "state_or_city=configParser.get('dataset-generation','state_or_city')\n",
        "city_dataset= configParser.get('dataset-generation','city')\n",
        "state_dataset= configParser.get('dataset-generation','state')\n",
        "equal_or_not=int(configParser.get('dataset-generation','equal_to'))\n",
        "\n",
        "if state_or_city=='state':\n",
        "  if equal_or_not==1:\n",
        "    location_df=location_df[location_df['geolocation_state']==state_dataset]\n",
        "  else:\n",
        "    location_df=location_df[location_df['geolocation_state']!=state_dataset]\n",
        "else:\n",
        "  if equal_or_not==1:\n",
        "    location_df=location_df[location_df['geolocation_city']==city_dataset]\n",
        "  else:\n",
        "    location_df=location_df[location_df['geolocation_city']!=city_dataset]\n",
        "\n",
        "\n",
        "location_df=location_df.drop_duplicates('geolocation_zip_code_prefix')\n",
        "\n",
        "\n",
        "training_data_percent=int(configParser.get('dataset-generation','training_data_percent'))\n",
        "club_num_dates=int(configParser.get('dataset-generation','club_num_dates'))\n",
        "\n",
        "def club_date(dt,club_num_dates):\n",
        "  return pd.datetime(dt.year,dt.month,int(dt.day/club_num_dates)*club_num_dates+1) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Dividing training and testing data\n",
        "#order_df Datefram having data related to orders\n",
        "order_df=order_df[order_df['order_status']=='delivered']\n",
        "order_df=order_df[order_df['order_delivered_customer_date'].notna()]\n",
        "order_df['order_delivered_customer_date']=pd.to_datetime(order_df['order_delivered_customer_date'])\n",
        "order_df['order_delivered_customer_date']=order_df['order_delivered_customer_date'].dt.date\n",
        "#Clubbing multiple dates into one, because the data for one date is too small\n",
        "order_df['order_delivered_customer_date']=order_df['order_delivered_customer_date'].map(lambda x: club_date(x,club_num_dates))\n",
        "order_df=order_df.sort_values(\"order_delivered_customer_date\")\n",
        "order_df=order_df.reset_index()\n",
        "\n",
        "\n",
        "\n",
        "overall_customer_df=pd.merge(customer_df,order_df,on='customer_id')\n",
        "overall_customer_loc_df=pd.merge(overall_customer_df,location_df,left_on='customer_zip_code_prefix' ,right_on='geolocation_zip_code_prefix')\n",
        "\n",
        "#Dividing training data and testing data\n",
        "start_training_date=order_df[\"order_delivered_customer_date\"].iloc[int((10+training_data_percent)/100*len(order_df.index))]\n",
        "end_training_date=order_df[\"order_delivered_customer_date\"].iloc[int(0.1*len(order_df.index))]\n",
        "training_order_df=order_df[order_df['order_delivered_customer_date']<=start_training_date]\n",
        "training_order_df=training_order_df[order_df['order_delivered_customer_date']>=end_training_date]\n",
        "num_training_days=training_order_df['order_delivered_customer_date'].nunique()\n",
        "\n",
        "\n",
        "training_customer_df=pd.merge(customer_df,training_order_df,on='customer_id')\n",
        "training_customer_df=training_customer_df.drop_duplicates('customer_unique_id')\n",
        "temp_df=pd.merge(order_item_df,training_order_df,on='order_id')\n",
        "training_seller_df=pd.merge(seller_df,temp_df,on='seller_id')\n",
        "training_seller_df=training_seller_df.drop_duplicates('seller_zip_code_prefix')\n",
        "\n",
        "\n",
        "training_customer_loc_df = pd.merge(training_customer_df, location_df,left_on = 'customer_zip_code_prefix' ,right_on='geolocation_zip_code_prefix')\n",
        "training_seller_loc_df = pd.merge(training_seller_df, location_df,left_on = 'seller_zip_code_prefix' ,right_on='geolocation_zip_code_prefix')\n",
        "\n",
        "training_customer_loc_df.rename(columns = {'customer_city':'city'}, inplace = True)\n",
        "training_seller_loc_df.rename(columns = {'seller_city':'city'}, inplace = True)\n",
        "   \n",
        "   \n",
        "\n",
        "\n",
        "training_customer_locations=training_customer_loc_df[['geolocation_lat','geolocation_lng','city']]\n",
        "training_seller_locations=training_seller_loc_df[['geolocation_lat','geolocation_lng','city']]\n",
        "training_combined_locations=pd.concat([training_customer_locations,training_seller_locations])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 777
        },
        "id": "J9qQYSMHCm6-",
        "outputId": "4bf1da85-401c-475f-8d89-e5643a828718"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Plot customer, sellers and drivers\n",
        "lis=['geolocation_lat','geolocation_lng']\n",
        "X=training_customer_locations[lis].values\n",
        "X1=np.transpose(X)\n",
        "plt.scatter(X1[1],X1[0],color='Red',label='Customers')\n",
        "X=training_seller_locations[lis].values\n",
        "X1=np.transpose(X)\n",
        "plt.scatter(X1[1],X1[0],color='Blue',label='Sellers')\n",
        "plt.legend()\n",
        "plt.xlabel('Longitude')\n",
        "plt.ylabel('Latitude')\n",
        "plt.title('Customers and Sellers locations')\n",
        "plt.show()\n",
        "X=training_combined_locations[lis].values\n",
        "print(\"Number of customers = \" + str(len(training_customer_locations.index)))\n",
        "print(\"Number of sellers = \" + str(len(training_seller_locations.index)))\n",
        "\n",
        "\n",
        "#Generating FFCs locations using customer and seller data locations\n",
        "configParser.read(configFilePath)\n",
        "num_ffc_center=int(configParser.get('dataset-generation','num_ffc_center'))\n",
        "kmeans=KMeans(n_clusters=num_ffc_center,random_state=0).fit(X)\n",
        "centre=kmeans.cluster_centers_\n",
        "label=pd.DataFrame(kmeans.labels_,columns=['label'])\n",
        "label['freq']=label.groupby('label')['label'].transform('count')\n",
        "label=label.drop_duplicates('label')\n",
        "label=label.reset_index(drop=True)\n",
        "label=label.sort_values('label')\n",
        "X1=np.transpose(centre)\n",
        "label['x']=X1[0]\n",
        "label['y']=X1[1]\n",
        "\n",
        "\n",
        "#Plot customer, sellers and drivers\n",
        "lis=['geolocation_lat','geolocation_lng']\n",
        "X=training_customer_locations[lis].values\n",
        "X1=np.transpose(X)\n",
        "plt.scatter(X1[1],X1[0],color='Red',label='Customers')\n",
        "X=training_seller_locations[lis].values\n",
        "X1=np.transpose(X)\n",
        "plt.scatter(X1[1],X1[0],color='Blue',label='Sellers')\n",
        "X1=np.transpose(centre)\n",
        "plt.scatter(X1[1],X1[0],color='Black',label='FF centres')\n",
        "plt.legend()\n",
        "plt.xlabel('Longitude')\n",
        "plt.ylabel('Latitude')\n",
        "plt.title('Customers, Sellers and Warehouses locations')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVAdKyfMnPS9"
      },
      "outputs": [],
      "source": [
        "#Separating testing data\n",
        "testing_order_df=order_df[order_df['order_delivered_customer_date']>end_training_date]\n",
        "testing_customer_df=pd.merge(customer_df,testing_order_df,on='customer_id')\n",
        "testing_customer_df=testing_customer_df.drop_duplicates('customer_unique_id')\n",
        "testing_customer_loc_df = pd.merge(testing_customer_df, location_df,left_on = 'customer_zip_code_prefix' ,right_on='geolocation_zip_code_prefix')\n",
        "\n",
        "testing_seller_df=pd.merge(seller_df,temp_df,on='seller_id')\n",
        "testing_seller_df=testing_seller_df.drop_duplicates('seller_zip_code_prefix')\n",
        "testing_seller_loc_df = pd.merge(testing_seller_df, location_df,left_on = 'seller_zip_code_prefix' ,right_on='geolocation_zip_code_prefix')\n",
        "\n",
        "\n",
        "\n",
        "# removing outliers\n",
        "lat_mean=training_combined_locations['geolocation_lat'].mean()\n",
        "lat_std= training_combined_locations['geolocation_lat'].std()\n",
        "testing_customer_loc_df=testing_customer_loc_df[(testing_customer_loc_df['geolocation_lat']-lat_mean)< 3*lat_std]\n",
        "testing_customer_loc_df=testing_customer_loc_df[(testing_customer_loc_df['geolocation_lat']-lat_mean)> -3*lat_std]\n",
        "\n",
        "lng_mean=training_combined_locations['geolocation_lng'].mean()\n",
        "lng_std= training_combined_locations['geolocation_lng'].std()\n",
        "testing_customer_loc_df=testing_customer_loc_df[(testing_customer_loc_df['geolocation_lng']-lng_mean)< 3*lng_std]\n",
        "testing_customer_loc_df=testing_customer_loc_df[(testing_customer_loc_df['geolocation_lng']-lng_mean)> -3*lng_std]\n",
        "\n",
        "testing_customer_loc_df=testing_customer_loc_df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "#Assigning ffc center to each customer as the nearest center according to prediction of kmeans distance\n",
        "lis=['geolocation_lat','geolocation_lng']\n",
        "prediction_array=np.array(kmeans.predict(testing_customer_loc_df[lis].values))\n",
        "unique, counts = np.unique(prediction_array, return_counts=True)\n",
        "testing_customer_loc_df['ffc_index']=prediction_array\n",
        "testing_customer_loc_df=testing_customer_loc_df.sort_values('order_delivered_customer_date')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Finding maximum and minimum capacity of a warehouse for training data\n",
        "prediction_array=np.array(kmeans.predict(training_customer_loc_df[lis].values))\n",
        "unique, counts = np.unique(prediction_array, return_counts=True)\n",
        "training_customer_loc_df['ffc_index']=prediction_array\n",
        "\n",
        "# Finding the number of orders of particular ffc center for each data\n",
        "training_customer_group_date = training_customer_loc_df.groupby(['ffc_index','order_delivered_customer_date'])['order_id'].count()\n",
        "\n",
        "configParser.read(configFilePath)\n",
        "quantile_capacity=float(configParser.get('dataset-generation','qauntile_capacity'))\n",
        "\n",
        "training_customer_group_date=pd.DataFrame({'count':training_customer_group_date}).reset_index()\n",
        "training_customer_group_date = training_customer_group_date.groupby(['ffc_index'])['count'].quantile(quantile_capacity)\n",
        "training_customer_group_date = pd.DataFrame({'cap':training_customer_group_date}).reset_index()\n",
        "\n",
        "training_customer_group_date['min_cap'] = 0.5 * training_customer_group_date['cap']\n",
        "training_customer_group_date['max_cap'] = 1.5 * training_customer_group_date['cap']\n",
        "training_customer_group_date=training_customer_group_date.sort_values('ffc_index')\n",
        "max_cap=training_customer_group_date['max_cap'].values\n",
        "min_cap=training_customer_group_date['min_cap'].values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "kfpCtYS7Z80E",
        "outputId": "80091956-89e4-45b3-9b62-6a5feb3702ee"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import random\n",
        "def driver_generation(val):\n",
        "\n",
        "  # Creating drivers through grid formation\n",
        "\n",
        "  X=training_combined_locations[lis].values\n",
        "  X=training_combined_locations[['geolocation_lat','geolocation_lng']].values\n",
        "  X1=np.transpose(X)\n",
        "  min_long=np.amin(X1[0])\n",
        "  max_long=np.amax(X1[0])\n",
        "\n",
        "  min_lat=np.amin(X1[1])\n",
        "  max_lat=np.amax(X1[1])\n",
        "\n",
        "  configParser.read(configFilePath)\n",
        "  prop_constant_driver_customer=float(configParser.get('dataset-generation','prop_constant_driver_customer'))\n",
        "  grid_length=int(configParser.get('dataset-generation','grid_length'))\n",
        "  grid_width=int(configParser.get('dataset-generation','grid_width'))\n",
        "  prop_constant_lower_range=float(configParser.get('dataset-generation','prop_constant_lower_range'))\n",
        "  prop_constant_higher_range=float(configParser.get('dataset-generation','prop_constant_upper_range'))\n",
        "\n",
        "  # random.seed(int(configParser.get('dataset-generation','Random_seed_value')))\n",
        "  random.seed(val)\n",
        "\n",
        "  grid_size=[grid_length,grid_width]\n",
        "  drivers=[[],[]]\n",
        "  sum_customer=0\n",
        "  sum_driver=0\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "  for i in range(grid_size[0]):\n",
        "    left_long=min_long+(max_long-min_long)/grid_size[0]*i\n",
        "    right_long=min_long+(max_long-min_long)/grid_size[0]*(i+1)\n",
        "    ind1=X1[0]>=left_long\n",
        "    ind2=X1[0]<=right_long\n",
        "\n",
        "    for j in range(grid_size[1]):\n",
        "      up_lat=min_lat+(max_lat-min_lat)/grid_size[1]*(j+1)\n",
        "      down_lat=min_lat+(max_lat-min_lat)/grid_size[1]*j\n",
        "      ind3=X1[1]>=down_lat\n",
        "      ind4=X1[1]<=up_lat\n",
        "      ind=ind1 & ind2 & ind3 & ind4\n",
        "     \n",
        "\n",
        "      num_customers=np.count_nonzero(ind)/num_training_days\n",
        "      num_drivers=num_customers/prop_constant_driver_customer\n",
        "      random_constant=random.uniform(prop_constant_lower_range,prop_constant_higher_range)\n",
        "      num_drivers=int(num_drivers*random_constant)\n",
        "\n",
        "      if num_drivers==0:\n",
        "        continue\n",
        "      for k in range(num_drivers):\n",
        "        drivers[0].append(random.uniform(left_long,right_long))\n",
        "        drivers[1].append(random.uniform(down_lat,up_lat))\n",
        "      \n",
        "\n",
        "      sum_driver=sum_driver+num_drivers\n",
        "      sum_customer=sum_customer+num_customers\n",
        "  drivers=np.array(drivers)\n",
        "\n",
        "  print(\"Number of Drivers:\",len(drivers[0]))\n",
        "  new_drivers=np.transpose(drivers)\n",
        "  \n",
        "\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  tempdf=testing_customer_loc_df\n",
        "  X=tempdf[lis].values\n",
        "  X1=np.transpose(X)\n",
        "  plt.scatter(X1[1],X1[0],color='Red',label='Customer')\n",
        "  X=testing_seller_loc_df[lis].values\n",
        "  X1=np.transpose(X)\n",
        "  plt.scatter(X1[1],X1[0],color='Blue',label='Seller')\n",
        "  X1=drivers\n",
        "  plt.scatter(X1[1],X1[0],color='Cyan',label='Drivers')\n",
        "  X1=np.transpose(centre)\n",
        "  plt.scatter(X1[1],X1[0],color='Black',label='FF centres')\n",
        "  plt.legend(loc='lower left',prop={'size':20})\n",
        "  plt.xlabel('Longitude')\n",
        "  plt.ylabel('Latitude')\n",
        "  # plt.savefig(\"Plots/Coordinate Plots/\"+state_dataset+\".pdf\")\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  return new_drivers\n",
        "\n",
        "\n",
        "\n",
        "driver_generation(12)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEuY2dhYKhGD",
        "outputId": "3d0bccf9-22c2-4b05-974b-39ad6ee8dde6"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Using max flow for the Vanilla Distribution of Drivers to the warehouse\n",
        "import networkx as nx\n",
        "from math import cos, asin, sqrt, pi\n",
        "\n",
        "def distance(lat1, lon1, lat2, lon2):\n",
        "    p = pi/180\n",
        "    a = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p) * cos(lat2*p) * (1-cos((lon2-lon1)*p))/2\n",
        "    return 12742 * asin(sqrt(a))\n",
        "\n",
        "\n",
        "configParser.read(configFilePath)\n",
        "prop_constant_driver_customer=float(configParser.get('dataset-generation','prop_constant_driver_customer'))\n",
        "\n",
        "upper_cap=max_cap/prop_constant_driver_customer\n",
        "lower_cap=min_cap/prop_constant_driver_customer\n",
        "lower_cap=lower_cap.astype(int)\n",
        "upper_cap=upper_cap.astype(int)\n",
        "upper_cap=np.array(upper_cap)\n",
        "lower_cap=np.array(lower_cap)\n",
        "lower_cap[lower_cap<=0]=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Code to generate MCCA distribution using networkX library\n",
        "def flow_with_max_capacity(driver_loc,centre):\n",
        "  G = nx.DiGraph()\n",
        "  num_variables=len(centre)+len(driver_loc)\n",
        "  for i in range(len(driver_loc)):\n",
        "    G.add_edge(1,i+2,capacity=1,weight=0)\n",
        "\n",
        "\n",
        "  for i in range(len(centre)):\n",
        "    G.add_edge(len(driver_loc)+1+(i+1),num_variables+2,capacity=int(upper_cap[i]),weight=0)\n",
        "\n",
        "\n",
        "\n",
        "  for i in range(len(driver_loc)):\n",
        "    for j in range(len(centre)):\n",
        "      dis=(driver_loc[i][0]-centre[j][0])**2+(driver_loc[i][1]-centre[j][1])**2\n",
        "      G.add_edge(i+2,len(driver_loc)+1+(j+1),capacity=1,weight=int(dis*1000))\n",
        "\n",
        "  mincostFlow = nx.max_flow_min_cost(G, 1, num_variables+2)\n",
        "  mincost = nx.cost_of_flow(G, mincostFlow)\n",
        "  print(mincost)\n",
        "  from networkx.algorithms.flow import maximum_flow\n",
        "  maxFlow = maximum_flow(G, 1, num_variables+2)[1]\n",
        "  print(nx.cost_of_flow(G, maxFlow))\n",
        "\n",
        "\n",
        "  print(nx.maximum_flow_value(G,1,num_variables+2))\n",
        "  driver_df=pd.DataFrame(driver_loc,columns=[\"geolocation_lat\",\"geolocation_lng\"])\n",
        "  driver_df['ffc_index']=-1\n",
        "  for i in range(len(driver_loc)):\n",
        "    for j in range(len(centre)):\n",
        "      if abs(mincostFlow[i+2][len(driver_loc)+(j+2)]-1)<0.1:\n",
        "        driver_df.at[i,'ffc_index']=j\n",
        "  return driver_df\n",
        "\n",
        "\n",
        "# Code to generate MCCA distribution using ortools library\n",
        "\n",
        "from ortools.graph import pywrapgraph\n",
        "def flow_with_maxmin_capacity(driver_loc,centre):\n",
        "  # Instantiate a SimpleMinCostFlow solver.\n",
        "  # Using min-max flow for the Vanilla Distribution of Drivers to the warehouse\n",
        "  num_variables=len(centre)+len(driver_loc)\n",
        "  min_cost_flow = pywrapgraph.SimpleMinCostFlow()\n",
        "  for i in range(len(driver_loc)):\n",
        "    min_cost_flow.AddArcWithCapacityAndUnitCost(1,i+2,1,0)\n",
        "\n",
        "  for i in range(len(driver_loc)):\n",
        "    for j in range(len(centre)):\n",
        "      dis=(driver_loc[i][0]-centre[j][0])**2+(driver_loc[i][1]-centre[j][1])**2\n",
        "      min_cost_flow.AddArcWithCapacityAndUnitCost(i+2,len(driver_loc)+1+(j+1),1,int(dis*1000))\n",
        "\n",
        "\n",
        "  for i in range(len(centre)):\n",
        "    min_cost_flow.AddArcWithCapacityAndUnitCost(len(driver_loc)+1+(i+1),num_variables+2,int(upper_cap[i])-int(lower_cap[i]),0)\n",
        "\n",
        "\n",
        "  for i in range(0,len(centre)):\n",
        "    min_cost_flow.SetNodeSupply(len(driver_loc)+1+(i+1),-int(lower_cap[i]))\n",
        "\n",
        "  source_supply=int(len(driver_loc))\n",
        "  sink_supply=int(len(driver_loc)-np.sum(lower_cap))\n",
        "\n",
        "  min_cost_flow.SetNodeSupply(1,source_supply)\n",
        "  min_cost_flow.SetNodeSupply(num_variables+2,-sink_supply)\n",
        "\n",
        "\n",
        "  for i in range(0,len(driver_loc)):\n",
        "    min_cost_flow.SetNodeSupply(i+2,0)\n",
        "\n",
        "\n",
        "  if min_cost_flow.Solve() == min_cost_flow.OPTIMAL:\n",
        "      print('Minimum cost:', min_cost_flow.OptimalCost())\n",
        "      print('')\n",
        "      print('  Arc    Flow / Capacity  Cost')\n",
        "      for i in range(min_cost_flow.NumArcs()):\n",
        "        if min_cost_flow.Flow(i)!=0:\n",
        "          cost = min_cost_flow.Flow(i) * min_cost_flow.UnitCost(i)\n",
        "          print('%1s -> %1s   %3s  / %3s       %3s' % (\n",
        "              min_cost_flow.Tail(i),\n",
        "              min_cost_flow.Head(i),\n",
        "              min_cost_flow.Flow(i),\n",
        "              min_cost_flow.Capacity(i),\n",
        "              cost))\n",
        "  else:\n",
        "    print('There was an issue with the min cost flow input.')\n",
        "\n",
        "  driver_df1=pd.DataFrame(driver_loc,columns=[\"geolocation_lat\",\"geolocation_lng\"])\n",
        "  driver_df1['ffc_index']=-1\n",
        "  for i in range(min_cost_flow.NumArcs()):\n",
        "    if min_cost_flow.Flow(i)!=0 and min_cost_flow.Tail(i)!=1 and min_cost_flow.Head(i)!=(num_variables+2):\n",
        "      tail=min_cost_flow.Tail(i)\n",
        "      head=min_cost_flow.Head(i)\n",
        "      driver_df1.at[tail-2,'ffc_index']=head-len(driver_loc)-2\n",
        "\n",
        "  return driver_df1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "Dkp5iSoyG5oe",
        "outputId": "ee6c9a1c-94a4-4928-b536-005ce8640e4d"
      },
      "outputs": [],
      "source": [
        "#Fair Clustering - LPP contstraints and Cplex\n",
        "from cplex import Cplex\n",
        "from lp_tools import *\n",
        "\n",
        "configParser.read(configFilePath)\n",
        "fair_distance=int(configParser.get('fairness-constraint','fair_distance'))\n",
        "alpha_fair =float(configParser.get('fairness-constraint','alpha_fair'))\n",
        "  \n",
        "\n",
        "\n",
        "def fair_clustering(dataset, centres):\n",
        "\n",
        "  # Step 1: \tCreate an instance of Cplex \n",
        "  problem = Cplex()\n",
        "\n",
        "  # Step 2: \tDeclare that this is a minimization problem\n",
        "  problem.objective.set_sense(problem.objective.sense.minimize)\n",
        "\n",
        "  # Step 3.   Declare and  add variables to the model. The function\n",
        "  #           prepare_to_add_variables (dataset, centres) prepares all the\n",
        "  #           required information for this stage.\n",
        "  #\n",
        "  #    objective: a list of coefficients (float) in the linear objective function\n",
        "  #    lower bound: a list of floats containing the lower bounds for each variable\n",
        "  #    upper bound: a list of floats containing the upper bounds for each variable\n",
        "  #    variable_names: a list of strings that contains the name of the variables\n",
        "\n",
        "  objective, lower_bound, upper_bound, variable_names, P,C = prepare_to_add_variables(dataset, centres)\n",
        "  problem.variables.add(\n",
        "      obj = objective,\n",
        "      lb = lower_bound,\n",
        "      ub = upper_bound,\n",
        "      names = variable_names\n",
        "     \n",
        "    )\n",
        "\n",
        "  # Step 4.   Declare and add constraints to the model.\n",
        "  #           There are few ways of adding constraints: row wise, col wise and non-zero entry wise.\n",
        "  #           Assume the constraint matrix is A. We add the constraints non-zero entry wise.\n",
        "  #           The function prepare_to_add_constraints(dataset, centres)\n",
        "  #           prepares the required data for this step.\n",
        "  #\n",
        "  #  coefficients: Three tuple containing the row number, column number and the value of the constraint matrix\n",
        "  #  senses: a list of strings that identifies whether the corresponding constraint is\n",
        "  #          an equality or inequality. \"E\" : equals to (=), \"L\" : less than (<=), \"G\" : greater than equals (>=)\n",
        "  #  rhs: a list of floats corresponding to the rhs of the constraints.\n",
        "  #  constraint_names: a list of string corresponding to the name of the constraint\n",
        " \n",
        "  rhs, senses, row_names, coefficients = prepare_to_add_constraints(dataset, centres, upper_cap,lower_cap, P,C, alpha_fair,fair_distance)\n",
        "  print(\"NUM_constraints\",len(senses))\n",
        "  problem.linear_constraints.add(\n",
        "      rhs = rhs,\n",
        "      senses = senses,\n",
        "      names = row_names\n",
        "    )\n",
        "  problem.linear_constraints.set_coefficients(coefficients)\n",
        "\n",
        "  # Step 5.\tSolve the problem\n",
        "  problem.solve()\n",
        "\n",
        "  result = {\n",
        "    \"status\": problem.solution.get_status(),\n",
        "    \"success\": problem.solution.get_status_string(),\n",
        "    \"objective\": problem.solution.get_objective_value(),\n",
        "    \"assignment\": problem.solution.get_values(),\n",
        "  }\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#Income Distribution Graph for Warehouses\n",
        "\n",
        "configParser.read(configFilePath)\n",
        "\n",
        "# Generates box plot for 2d income list\n",
        "def box_plot(income_2d,city_lis):\n",
        "  fig, ax = plt.subplots()\n",
        "  plt.boxplot(income_2d)\n",
        "  ax.set_xticklabels(city_lis,rotation='vertical')\n",
        "  plt.ylabel('Number of Deliveries')\n",
        "  plt.ylim((0,1000))\n",
        "  ax.set_ylim([0, 700])\n",
        "  plt.show()\n",
        "  \n",
        "\n",
        "# Generates scatter income plot, labeling drivers according to the range in which their income lies\n",
        "def scatter_income(driver_income_df,fair_type):\n",
        "  fig, ax = plt.subplots()\n",
        "  fig.set_size_inches(13, 9)\n",
        "  configParser.read(configFilePath)\n",
        "\n",
        "  diff_num=int(configParser.get('results-parameter','diff_num'))\n",
        "  driver_income_df=driver_income_df.sort_values('total_income')\n",
        "  driver_income_df=driver_income_df.reset_index()\n",
        "\n",
        "  df_len=len(driver_income_df.index)\n",
        "  low_list=[0,45,90,135]\n",
        "  high_list=[45,90,135,100000]\n",
        "  colors=['red','orange','green','blue']\n",
        "\n",
        "  for i in range(diff_num):\n",
        "    low_val=low_list[i]\n",
        "    high_val=high_list[i]\n",
        "    print(low_val,high_val)\n",
        "    ind1=(driver_income_df['total_income']<=high_val)\n",
        "    ind2=(driver_income_df['total_income']>low_val)\n",
        "    tempo_df=driver_income_df[ind1 & ind2]\n",
        "\n",
        "    label_string=str(low_val)\n",
        "    if i==3:\n",
        "      label_string=label_string+\"+\"\n",
        "    else:\n",
        "      label_string=label_string+\" - \"+str(high_val)\n",
        "    \n",
        "    plt.scatter(tempo_df['geolocation_lng'],tempo_df['geolocation_lat'],color=colors[i],label=label_string)\n",
        "  plt.legend(loc='upper left',prop={'size':20})\n",
        "  plt.xlabel('Longitude',fontsize='26')\n",
        "  plt.ylabel('Latitude',fontsize='26')\n",
        "\n",
        "  if fair_type==\"fair_algo\":\n",
        "    plt.savefig(\"Plots/Scatter Plots/\"+state_dataset+\"_result.pdf\")\n",
        "  else:\n",
        "    plt.savefig(\"Plots/Scatter Plots/\"+state_dataset+\"_motivation.pdf\")\n",
        "  print(driver_income_df['total_income'].values)\n",
        "  \n",
        "\n",
        "  plt.show()\n",
        "    \n",
        "\n",
        "\n",
        "def lorenz(arrs,name_dis,colors):\n",
        "    # this divides the prefix sum by the total sum\n",
        "    # this ensures all the values are between 0 and 1.0\n",
        "    import matplotlib.pyplot as plt1\n",
        "    fig, ax = plt1.subplots()\n",
        "    fig.set_size_inches(10, 7.5)\n",
        "    plt1.rcParams['font.size'] = '26'\n",
        "    plt1.rcParams['figure.figsize'] = (10,7.5)\n",
        "    \n",
        "\n",
        "    for i in range(len(arrs)):\n",
        "      arr=arrs[i]\n",
        "      arr=np.array(arr)\n",
        "      arr=np.sort(arr)\n",
        "      plt.rcParams.update({'font.size': 14})\n",
        "      scaled_prefix_sum = arr.cumsum() / arr.sum()*100\n",
        "      # this prepends the 0 value (because 0% of all people have 0% of all wealth)\n",
        "      lorenz_cruve= np.insert(scaled_prefix_sum, 0, 0)\n",
        "      # we need the X values to be between 0.0 to 1.0\n",
        "\n",
        "      plt1.plot(np.linspace(0.0, 100.0, lorenz_cruve.size), lorenz_cruve,label=name_dis[i],color=colors[i])\n",
        "    \n",
        "    plt1.plot([0,100],[0,100],color='Black',linestyle='--')\n",
        "    plt1.legend(loc='upper left',prop={'size':20})\n",
        "   \n",
        "    plt1.xlabel('% of all drivers',fontsize=26)\n",
        "    plt1.ylabel('% of total deliveries',fontsize=26)\n",
        "    if len(name_dis)==2:\n",
        "      plt1.savefig(\"Plots/Lorenz Curves/\"+state_dataset+\"_motivation.pdf\")\n",
        "    else:\n",
        "      plt1.savefig(\"Plots/Lorenz Curves/\"+state_dataset+\"_result.pdf\")\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "# Caluclate average distance each driver has to travel each day from going to home to the location of FF Center\n",
        "def average_distance(driver_dist,label):   \n",
        "    distance_warehouse_driver_df = pd.merge(driver_dist, label,left_on = 'ffc_index' ,right_on='label')\n",
        "    dist1=0.0\n",
        "    for i in range(len(distance_warehouse_driver_df)):\n",
        "        dist1 = dist1 +math.sqrt((distance_warehouse_driver_df.loc[i].at[\"geolocation_lat\"] - distance_warehouse_driver_df.loc[i].at[\"x\"])**2 + (distance_warehouse_driver_df.loc[i].at[\"geolocation_lng\"] - distance_warehouse_driver_df.loc[i].at[\"y\"])**2)\n",
        "    return round(dist1/len(distance_warehouse_driver_df),2)\n",
        "\n",
        "\n",
        "    \n",
        "# Returns gini index on income list\n",
        "def gini_index(incomes):\n",
        "  driver_income=np.array(incomes)\n",
        "  driver_income=np.sort(driver_income)\n",
        "  integer_array=np.array([(i+1) for i in range(len(driver_income))])\n",
        "  driver_income=driver_income*750\n",
        "  gini_index=2*(np.dot(driver_income,integer_array))/(len(driver_income)*(np.sum(driver_income))) - 1 -1/(len(driver_income))\n",
        "  return round(gini_index,3)\n",
        "\n",
        "\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def inequality(df,date_index):\n",
        "  #Finding the number of drivers assigned to every ff center\n",
        "  df['driver_index']=np.arange(len(df.index))  \n",
        "  driver_assign = df.groupby('ffc_index')['geolocation_lat'].count()\n",
        "  driver_assign=pd.DataFrame({'num_drivers':driver_assign}).reset_index()\n",
        "\n",
        "  # Adding 0 driver count to center to whom no driver was assigned\n",
        "  for i in range(len(centre)):\n",
        "    if i not in driver_assign['ffc_index'].values:\n",
        "      driver_assign.loc[len(driver_assign.index)]=[i,0]\n",
        "\n",
        "\n",
        "  driver_assign=driver_assign.sort_values('ffc_index')\n",
        "  driver_assign['min_cap']=lower_cap\n",
        "  driver_assign['max_cap']=upper_cap\n",
        "\n",
        "  # Finding income of each driver as total number of customers divided by total number of drivers in that center\n",
        "  # For a particular data: dates[date_index] \n",
        "\n",
        "  #Finding the income of each warehouse\n",
        "  testing_customer_group_date = testing_customer_loc_df.groupby(['ffc_index','order_delivered_customer_date'])['order_id'].count()\n",
        "  testing_customer_group_date = pd.DataFrame({'num_delivery_per_date':testing_customer_group_date}).reset_index()\n",
        "\n",
        "  dates=testing_customer_group_date['order_delivered_customer_date'].unique()\n",
        "  testing_customer_group_date=testing_customer_group_date[testing_customer_group_date['order_delivered_customer_date']==dates[date_index]]\n",
        "\n",
        "  warehouse_income_df = pd.merge(testing_customer_group_date, driver_assign,on='ffc_index',how='right')\n",
        "  warehouse_income_df['income_warehouse_driver'] = warehouse_income_df['num_delivery_per_date'] / warehouse_income_df['num_drivers']\n",
        "  warehouse_income_df.loc[~np.isfinite(warehouse_income_df['income_warehouse_driver']), 'income_warehouse_driver'] = 0\n",
        "  warehouse_income_df=warehouse_income_df.round(2)\n",
        "  warehouse_income_df.loc[warehouse_income_df['num_drivers']==0,'income_warehouse_driver']=0\n",
        "  warehouse_income_df.reset_index(inplace=True)\n",
        "\n",
        "  warehouse_incomes=warehouse_income_df['income_warehouse_driver'].values\n",
        "\n",
        "  df['date']=dates[date_index]\n",
        "  df['income']=-1.0\n",
        "  \n",
        "\n",
        "  # Assigning the income of ff center to which the driver was assigned\n",
        "  for i in range(len(df.index)):\n",
        "    df.at[i,'income']=warehouse_income_df.at[df.iloc[i]['ffc_index'],'income_warehouse_driver']\n",
        "  return df,warehouse_incomes\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Fair Assignment of drivers to the FFCs/ warehouses\n",
        "import dependent_routing as dp\n",
        "configParser.read(configFilePath)\n",
        "\n",
        "# Assigning the driver using the probaility distribution using dependent routing\n",
        "def fair_assignment(prob_dis,driver_loc):\n",
        "  \n",
        "  final_ans1=copy.deepcopy(prob_dis)\n",
        "  print(final_ans1.shape)\n",
        "  routing=dp.DependentRounding(final_ans1)\n",
        "  routing._buildGraph(final_ans1)\n",
        "  final_assignment=routing.round()\n",
        "  final_assignment=np.around(final_assignment,2)\n",
        "\n",
        "  driver_df2=pd.DataFrame(driver_loc,columns=[\"geolocation_lat\",\"geolocation_lng\"])\n",
        "  driver_df2['ffc_index']=-1\n",
        "  for i in range(num_samples):\n",
        "    for j in range(num_centres):\n",
        "      if abs(final_assignment[i][j]-1)<0.01:\n",
        "        driver_df2.at[i,'ffc_index']=j\n",
        "  return driver_df2,final_assignment\n",
        "\n",
        "\n",
        "# Randomly assigning the drivers to ff center keeping the upper_cap constraint\n",
        "def random_dist(driver_loc):\n",
        "  driver_df4=pd.DataFrame(driver_loc,columns=[\"geolocation_lat\",\"geolocation_lng\"])\n",
        "  driver_df4['ffc_index']=-1\n",
        "  temp_upper_cap=list(upper_cap)\n",
        "  for i in range(num_samples):\n",
        "    ffc=random.randint(1,num_centres)-1\n",
        "    while(temp_upper_cap[ffc]<=0 ):\n",
        "      ffc=random.randint(1,num_centres)-1\n",
        "    driver_df4.at[i,'ffc_index']=ffc\n",
        "    temp_upper_cap[ffc]=temp_upper_cap[ffc]-1\n",
        "  return driver_df4\n",
        "\n",
        "# Assign the drivers in round robin manner to ff center keeping the upper_cap constraint\n",
        "def round_robin_dist(d,driver_loc):\n",
        "  driver_df5=pd.DataFrame(driver_loc,columns=[\"geolocation_lat\",\"geolocation_lng\"])\n",
        "  driver_df5['ffc_index']=-1\n",
        "  temp_upper_cap=list(upper_cap)\n",
        "  for i in range(num_samples):\n",
        "    ffc=(d+i)%num_centres\n",
        "    while(temp_upper_cap[ffc]<=0):\n",
        "      ffc=(ffc+1)%num_centres\n",
        "    driver_df5.at[i,'ffc_index']=ffc\n",
        "    temp_upper_cap[ffc]=temp_upper_cap[ffc]-1\n",
        "  return driver_df5\n",
        "\n",
        "# Assign the drivers with LIPA manner to ff center keeping the upper_cap constraint\n",
        "def low_income_dist(prev_income_drivers,prev_incomes_warehouse,driver_loc,lpp_prob_dis):\n",
        "  if len(prev_income_drivers)==0:\n",
        "    return fair_assignment(lpp_prob_dis,driver_loc)[0]\n",
        "  driver_df6=pd.DataFrame(driver_loc,columns=[\"geolocation_lat\",\"geolocation_lng\"])\n",
        "  driver_df6['ffc_index']=-1\n",
        "\n",
        "  temp_upper_cap=list(upper_cap)\n",
        "  driver_index_inc=np.argsort(np.array(prev_income_drivers))\n",
        "  warehouse_index_inc=np.argsort(np.array(prev_incomes_warehouse))\n",
        "  j=num_centres-1\n",
        "  for i in driver_index_inc:\n",
        "    ffc=warehouse_index_inc[j]\n",
        "    while(temp_upper_cap[ffc]<=0):\n",
        "      j=j-1\n",
        "      ffc=warehouse_index_inc[j]\n",
        "    driver_df6.at[i,'ffc_index']=ffc\n",
        "    temp_upper_cap[ffc]=temp_upper_cap[ffc]-1\n",
        "  return driver_df6\n",
        "\n",
        "# Returns the total cost of the drivers from their location to ff center\n",
        "def total_cost(df):\n",
        "  sum=0\n",
        "  for index,row in df.iterrows():\n",
        "    sum=sum+ np.sqrt( (row['geolocation_lat']-centre[int(row['ffc_index'])][0])**2 + (row['geolocation_lng']-centre[int(row['ffc_index'])][1])**2 )\n",
        "  return round(sum,2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Finding the income of all drivers for 30 days, with given distribution type\n",
        "def inequality2(algo_type=\"random\",num_days=30,lpp_prob_dis=[[]]):\n",
        "  fair_assignment_totaldays=None\n",
        "  overall_cost=0\n",
        "  prev_income_drivers=[]\n",
        "  prev_incomes_warehouse=[]\n",
        "  df=None\n",
        "  if algo_type==\"vanilla_max\":\n",
        "    df_vanilla_max=flow_with_max_capacity(new_drivers,centre)\n",
        "  if algo_type==\"vanilla_maxmin\":\n",
        "    df_vanilla_maxmin=flow_with_maxmin_capacity(new_drivers,centre)\n",
        "\n",
        "  # Calling appropriate function according to the given algo_type paramtric value\n",
        "  for i in range(num_days):\n",
        "    if algo_type==\"random\": df=random_dist(new_drivers)\n",
        "    if algo_type==\"round_robin\": df=round_robin_dist(i,new_drivers)\n",
        "    if algo_type==\"fair_algo\": df=fair_assignment(lpp_prob_dis,new_drivers)[0]\n",
        "    if algo_type==\"vanilla_max\": df=df_vanilla_max   \n",
        "    if algo_type==\"vanilla_maxmin\": df=df_vanilla_maxmin  \n",
        "    if algo_type==\"low_income_dist\":df=low_income_dist(prev_income_drivers,prev_incomes_warehouse,new_drivers,lpp_prob_dis)\n",
        "    \n",
        "\n",
        "    df_result,prev_incomes_warehouse=inequality(df,i)\n",
        "    overall_cost=overall_cost+total_cost(df)\n",
        "\n",
        "    if fair_assignment_totaldays is None:\n",
        "      fair_assignment_totaldays=df_result\n",
        "    else:\n",
        "      fair_assignment_totaldays=pd.concat([fair_assignment_totaldays,df_result])\n",
        "    prev_income_drivers=fair_assignment_totaldays.groupby('driver_index')['income'].sum().values\n",
        "\n",
        "\n",
        "  # Finding total income of drivers over all days  \n",
        "  lis1=fair_assignment_totaldays.groupby('driver_index')['income'].sum().values\n",
        "  fair_assignment_totaldays['total_income'] = fair_assignment_totaldays['income'].groupby(fair_assignment_totaldays['driver_index']).transform('sum')\n",
        "  \n",
        "  driver_income_df=fair_assignment_totaldays.groupby('driver_index').first().reset_index()  \n",
        "  avg_distance=average_distance(fair_assignment_totaldays,label)\n",
        "  return gini_index(lis1),avg_distance,fair_assignment_totaldays,np.sum(np.array(lis1)),lis1\n",
        "\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Find the income gap per pair of those drivers which are separated by less than fair_distance\n",
        "def helper_func(table,driver_loc):\n",
        "    income_gap = 0.0\n",
        "    assignment_total = table.groupby(['driver_index','geolocation_lat','geolocation_lng'])['income'].sum()\n",
        "    assignment_total=pd.DataFrame({'income_sum':assignment_total}).reset_index()\n",
        "    income_array = []\n",
        "    for i in range(len(driver_loc)):\n",
        "        income_array.append(assignment_total.loc[i].at[\"income_sum\"])\n",
        "    income_array = np.array(income_array)\n",
        "\n",
        "    transposed = np.expand_dims(driver_loc, axis = 1)\n",
        "    distance_drivers = np.power(driver_loc - transposed, 2)\n",
        "    distance_drivers = np.power(np.abs(distance_drivers).sum(axis = 2), 0.5) \n",
        "    distance_drivers=distance_drivers*110\n",
        "\n",
        "    transposed = np.expand_dims(income_array, axis = 1)\n",
        "    income_diff = np.abs(income_array - transposed)\n",
        "\n",
        "    configParser.read(configFilePath)\n",
        "    fair_distance=int(configParser.get('fairness-constraint','fair_distance'))\n",
        "    \n",
        "    num_pair_fair_drivers=1e-7\n",
        "    for i in range(len(driver_loc)-1):\n",
        "        for j in range(i+1,len(driver_loc)):\n",
        "            if(distance_drivers[i][j] <=fair_distance):\n",
        "                income_gap = income_gap + float(income_diff[i][j])/distance_drivers[i][j]\n",
        "                num_pair_fair_drivers=num_pair_fair_drivers+1\n",
        "    \n",
        "    return round(income_gap/num_pair_fair_drivers,2),income_diff,distance_drivers,assignment_total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Returns spatial inequality.\n",
        "def spatial_inequality(income_diff, distance_drivers,table,driver_loc):\n",
        "    sum_i = 0.0\n",
        "    total_sum = 0.0\n",
        "    configParser.read(configFilePath)\n",
        "    fair_distance=int(configParser.get('fairness-constraint','fair_distance'))\n",
        "    for i in range(len(driver_loc)):\n",
        "        sum_i = sum_i + table.loc[i].at[\"income_sum\"]\n",
        "        sum_i_j = 0.0\n",
        "        num_i_j = 1e-7\n",
        "        for j in range(i+1,len(driver_loc)):\n",
        "            dist = distance_drivers[i][j]\n",
        "            if (dist < fair_distance):\n",
        "                sum_i_j = sum_i_j + income_diff[i][j]\n",
        "                num_i_j = num_i_j + 1\n",
        "        total_sum = total_sum + float(sum_i_j)/num_i_j\n",
        "    sp_index = float(total_sum)/sum_i\n",
        "    return round(sp_index,4)\n",
        "\n",
        "def fraction(base,fair):\n",
        "    fair_income_array=fair[\"income_sum\"].values\n",
        "    base_income_array=base[\"income_sum\"].values\n",
        "    return np.around((np.divide(fair_income_array,(base_income_array+1e-5))-1)*100,2)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Gini_Index_List=[]\n",
        "Avg_Distance_List=[]\n",
        "Income_Gap_List=[]\n",
        "spatial_inequality_list=[]\n",
        "Name_list=[]\n",
        "Income_sum_lis=[]\n",
        "num_drivers=[]\n",
        "income_lis_lis=[]\n",
        "fraction_list=[]\n",
        "import copy\n",
        "\n",
        "\n",
        "\n",
        "# Do a sanity check on the distribution we get from FairAssign algorithm\n",
        "# Flag if the value is less than -1e-5, otherwise for other negative values convert them to 0\n",
        "def sanityCheck(weights):\n",
        "    for i in range(len(weights)):\n",
        "        last_pos_index=-1\n",
        "        neg_value=0\n",
        "        for j in range(len(weights[0])):\n",
        "            assert weights[i][j]>-1e-5\n",
        "            if weights[i][j]<0:\n",
        "                neg_value+=weights[i][j]\n",
        "                weights[i][j]=0\n",
        "            elif weights[i][j]>0:\n",
        "                last_pos_index=j\n",
        "        weights[i][last_pos_index]+=neg_value\n",
        "    return weights\n",
        "\n",
        "\n",
        "configParser.read(configFilePath)\n",
        "number_of_runs=int(configParser.get('dataset-generation','number_of_runs'))\n",
        "random_seed=int(configParser.get('dataset-generation','random_seed'))\n",
        "\n",
        "dis_type=[\"random\",\"low_income_dist\",\"round_robin\",\"vanilla_max\",\"vanilla_maxmin\",\"fair_algo\"]\n",
        "Name_dis_types=[\"Random\",\"LIPA\",\"RoundRobin\",\"MCCA\",\"MCCA-L\",\"FairAssign\"]\n",
        "colors=['Indigo','Blue','Cyan','Green','Orange','Red']\n",
        "\n",
        "indexs_allowed=[0,1,2,3,4,5]\n",
        "\n",
        "# Generates result for #number_of_runs times, and store the value in differnt list\n",
        "for i in range(number_of_runs):\n",
        "    new_drivers=driver_generation(i+random_seed)\n",
        "    if len(new_drivers)<sum(lower_cap):\n",
        "        print(\"Lower number of drivers generated\")\n",
        "        print(len(new_drivers), sum(lower_cap))\n",
        "        continue\n",
        "\n",
        "    result=fair_clustering(new_drivers,centre)\n",
        "    num_samples = len(new_drivers)\n",
        "    num_centres = len(centre)\n",
        "\n",
        "    #Probability Distribution output from cplex\n",
        "    lpp_prob_dis=np.reshape(result['assignment'][:num_samples*num_centres],(-1,num_centres))\n",
        "    lpp_prob_dis=sanityCheck(copy.deepcopy(lpp_prob_dis))\n",
        "\n",
        "    print(\"Everything solved, graphs start\")\n",
        "    income_lis_lis=[]\n",
        "    \n",
        "\n",
        "    for j in indexs_allowed:\n",
        "        tup=inequality2(algo_type=dis_type[j],lpp_prob_dis=lpp_prob_dis)\n",
        "        tup2 = helper_func(tup[2],new_drivers)\n",
        "        spat=spatial_inequality(tup2[1],tup2[2],tup2[3],new_drivers)\n",
        "\n",
        "        Gini_Index_List.append(tup[0])\n",
        "        Avg_Distance_List.append(tup[1])\n",
        "        Income_Gap_List.append(tup2[0])\n",
        "        spatial_inequality_list.append(spat)\n",
        "        Name_list.append(Name_dis_types[j])\n",
        "        Income_sum_lis.append(tup[3])\n",
        "        num_drivers.append(num_samples)\n",
        "        income_lis_lis.append(tup[4])\n",
        "\n",
        "        if j==3:\n",
        "            base=tup2[3]\n",
        "        if j==5:\n",
        "            fair=tup2[3]\n",
        "\n",
        "    fraction_worse_off = fraction(base,fair)\n",
        "    fraction_list.append(fraction_worse_off)\n",
        "\n",
        "\n",
        "    All_Income=pd.DataFrame({'Dist Type':Name_list,'Gini': Gini_Index_List, 'Avg Dist':Avg_Distance_List,'Income_Gap':Income_Gap_List,'spatial_index':spatial_inequality_list})\n",
        "    print(All_Income.groupby('Dist Type').mean())\n",
        "    print()\n",
        "    print(All_Income.groupby('Dist Type').max())\n",
        "    print()\n",
        "    print(All_Income.groupby('Dist Type').min())\n",
        "    print()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the fractional number of people worse off or get better when applied FairAssign algorithm keeping base as MCCA\n",
        "from statistics import mean\n",
        "\n",
        "greater_than_threshold=[40,35,30,25,20,15,10,5,0]\n",
        "lesser_than_threshold=[0,-5,-10,-15,-20,-25,-30,-35,-40]\n",
        "greater_than_matrix=[[0 for j in range(len(fraction_list))] for i in range(len(greater_than_threshold))]\n",
        "lesser_than_matrix=[[0 for j in range(len(fraction_list))] for i in range(len(lesser_than_threshold))]\n",
        "\n",
        "for i in range(len(fraction_list)):\n",
        "    for j in range(len(greater_than_threshold)):\n",
        "        val=(fraction_list[i]>=greater_than_threshold[j]).sum()/len(fraction_list[i])\n",
        "        greater_than_matrix[j][i]=val\n",
        "for i in range(len(fraction_list)):\n",
        "    for j in range(len(lesser_than_threshold)):\n",
        "        val=(fraction_list[i]<lesser_than_threshold[j]).sum()/len(fraction_list[i])\n",
        "        lesser_than_matrix[j][i]=val\n",
        "\n",
        "avg_greater_than=[]\n",
        "avg_lesser_than=[]\n",
        "for i in range(len(greater_than_matrix)):\n",
        "    avg_greater_than.append(sum(greater_than_matrix[i])/len(greater_than_matrix[i]))\n",
        "for i in range(len(lesser_than_matrix)):\n",
        "    avg_lesser_than.append(sum(lesser_than_matrix[i])/len(lesser_than_matrix[i]))\n",
        "\n",
        "\n",
        "print(greater_than_threshold+lesser_than_threshold)\n",
        "\n",
        "Change_df=pd.DataFrame({'Change': greater_than_threshold+lesser_than_threshold,'Val':avg_greater_than+avg_lesser_than})\n",
        "print(Change_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Print all the result\n",
        "All_Income=pd.DataFrame({'Dist Type':Name_list,'Gini': Gini_Index_List, 'Avg Dist':Avg_Distance_List,'Income_Gap':Income_Gap_List,'spatial_index':spatial_inequality_list,'Income_sum':Income_sum_lis})\n",
        "\n",
        "All_Income=All_Income.replace(\"Low Income Distribution\",\"Robinhood\")\n",
        "configParser.read(configFilePath)\n",
        "print(\"State: \",configParser.get('dataset-generation','state'))\n",
        "print(\"Equal_to_or_not: \",configParser.get('dataset-generation','equal_to'))\n",
        "print(\"num_ffc_center: \",configParser.get('dataset-generation','num_ffc_center'))\n",
        "print(\"Grid Size: \",configParser.get('dataset-generation','grid_length'),configParser.get('dataset-generation','grid_width'))\n",
        "print(\"Fair Distance: \",configParser.get('fairness-constraint','fair_distance'))\n",
        "print(\"Alpha_fair: \",configParser.get('fairness-constraint','alpha_fair'))\n",
        "print(\"number_of_runs: \",configParser.get('dataset-generation','number_of_runs'))\n",
        "print()\n",
        "print(\"Avg Num of Drivers: \",np.mean(np.array(num_drivers)))\n",
        "\n",
        "\n",
        "print()\n",
        "print(All_Income.groupby('Dist Type').mean().round(4))\n",
        "print()\n",
        "print(All_Income.groupby('Dist Type').max())\n",
        "print()\n",
        "print(All_Income.groupby('Dist Type').min())\n",
        "print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "cluster.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "cbc1630f6ac27d00c9f4ea5ff59d07a3d2920d2fb3e683b05679c37215caa53f"
    },
    "kernelspec": {
      "display_name": "Python 3.7.0 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
